
# LLM chat bot - AI English Tutor 
In this project, I build a complete Q&A chatbot related to Vietnamese Ministry of Education English Textbook for Grade 12 (Global Success)

# Getting started

To get starte with this project, we need to do the following

## Prepare enviroment 
Install all dependencies dedicated to the project in local

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r backend/requirements.txt
pip install -r chatbot-ui/requirements.txt
```
Start application .
```bash
sh backend/entrypoint.sh
sh chatbot-ui/entrypoint.sh
```

# Application services 

## RAG (Retrieval-Augmented Generation) 

### System overview

### RAG flow answering
- **Query reflection**: Chat history and current query will be rewritten into a single sentence with more complete meaning for easier retrieval. The model used in this step is `llama-3.1`.

- **Retrieval Relevant Documents:**: The rewritten query will be passed through two embedding models - Multilingual-e5-small, then ChromaDb will be used to retrieve semantically related documents. Besides elasticsearch is also used for retrieval based on lexical matching. Finally, to avoid losing relevant documents during retrieval, all retrieved documents were merged and duplicates were removed.

- **Reranking:**: If reranking is not used, the number of retrieved documents is quite large. If this entire number of documents is put into ChatModel (as Openai), it may exceed the model's input token limit and be expensive. If the number of documents is small (small top_k), it may lead to the loss of related documents. The topk documents retrieved from the previous step will be passed through the rerank model to re-rank the scores and get the top5 documents with the highest scores.

- **Generating Final Answer**: The LLM combines the top5 documents after reranking step with the user's query and chat history to generate a response. In the prompt for LLM I specified that it will return 'no' if the retrieved document does not contain the answer, so if the response is different from 'no' then it will be the final answer. If the response is 'no' then it will call the search tool in the next step to get more information.

### Finetune rerank model
Create enviroment 
```bash
cd retrieval
sh setup_env.sh
```
#### Create data finetune
- Train data should be a json file, where each line is a dict like this:

```shell
{"query": str, "pos": List[str], "neg":List[str]}
```
`query` is the query, and `pos` is a list of positive texts, `neg` is a list of negative texts. 
- For each embedding model => will take the top 25 chunks with the highest similarity to each query. If the chunk is in the labeled data, it will be assigned as positive and vice versa, it will be negative => Then the results of these embedding models will be summarized.

- Follow the steps below to create the training dataset

```bash
Step1: cd retrieval
Step2: CUDA_VISIBLE_DEVICES=0 python create_data_rerank.py
```

#### Finetune Multilingual-e5
Finetune Multilingual-e5 with parameters: 

    - epochs: 6
    - learning_rate: 1e-5
    - batch_size = 2

Run script for training
```bash
sh finetune.sh
```
### Finetune LLM for answer generation
#### Create + format training data
- The training data will be in conversational format.
```shell
{"messages": [{"role": "system", "content": "You are..."}, {"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}
{"messages": [{"role": "system", "content": "You are..."}, {"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}
{"messages": [{"role": "system", "content": "You are..."}, {"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}
```
- Follow the steps below to create the training + test dataset
```bash
Step1: cd finetune_llm
Step2: python gen_data.py
```
- The number of training dataset are 10000 samples and the number of test dataset are 1000 samples

#### Finetune LLM
- The base model I used for finetune is [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct).
- I used the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from trl library to finetune this model. Beside, I user [QLora](https://arxiv.org/abs/2305.14314) technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance by using quantization.

Run script for training
```bash
CUDA_VISIBLE_DEVICES=0 python finetune.py
```

### Evaluate 

The evaluation metrics currently in use are:

- **Recall@k**: Evaluate the accuracy of information retrieval
- **Correctness**:The metric evaluates the answer generated by the system to match a given query reference answer.

# DEMO       
