{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_222780/1627859251.py:27: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
      "  llm = Gemini(model=\"models/gemini-2.0-flash\", temperature=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import json\n",
    "import chromadb\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex, \n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    Settings\n",
    ")\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core.schema import QueryBundle\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Initialize Gemini\n",
    "os.environ[\"GEMINI_API_KEY\"] = os.getenv('GEMINI_API_KEY')\n",
    "llm = Gemini(model=\"models/gemini-2.0-flash\", temperature=1)\n",
    "\n",
    "# Initialize embedding model\n",
    "lc_embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/multilingual-e5-small\"\n",
    ")\n",
    "embed_model = LangchainEmbedding(lc_embed_model)\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Initialize ChromaDB\n",
    "# chroma_client = chromadb.PersistentClient(path=\"./chromadb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"../chromadb\")\n",
    "\n",
    "def load_database():\n",
    "    \"\"\"Connect database with ChromaDB backend.\"\"\"\n",
    "    chroma_collection = chroma_client.get_collection(\"unit1_db\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store=vector_store,\n",
    "        embed_model=embed_model,\n",
    "    )\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.prompts import PromptTemplate\n",
    "# from llama_index.core.output_parsers import PydanticOutputParser\n",
    "# from prototype.config.settings import llm\n",
    "# from prototype.schemas.quiz import QuizQuestion\n",
    "\n",
    "# def generate_questions_batch(chunks, n_questions):\n",
    "#     \"\"\"Generates multiple questions from given chunks of text\"\"\"\n",
    "#     combined_content = \"\\n\\n\".join(chunks)\n",
    "#     output_parser = PydanticOutputParser(QuizQuestion)\n",
    "    \n",
    "#     prompt_template = PromptTemplate(\n",
    "#         template=\"\"\"\n",
    "#             Generate {n_questions} different multiple-choice questions based on the following content.\n",
    "#             Make sure questions cover different aspects and concepts from the content.\n",
    "#             Content: {content}\n",
    "            \n",
    "#             For each question, provide output in exactly this format:\n",
    "#             {format_instructions}\n",
    "            \n",
    "#             Generate exactly {n_questions} questions, with each question separated by two newlines.\n",
    "#             DO NOT return a JSON array. Return each question in the exact format specified above.\n",
    "#         \"\"\"\n",
    "#     )\n",
    "    \n",
    "#     prompt = prompt_template.format(\n",
    "#         content=combined_content,\n",
    "#         n_questions=n_questions,\n",
    "#         format_instructions=output_parser.get_format_string()\n",
    "#     )\n",
    "    \n",
    "#     response = llm.complete(prompt)\n",
    "#     return parse_questions(response.text, output_parser, n_questions)\n",
    "\n",
    "# def generate_explanation(question, correct_answer):\n",
    "#     \"\"\"Generates explanation for a question\"\"\"\n",
    "#     template = PromptTemplate(\n",
    "#         template=\"Provide an explanation for the following question and answer:\\n\\nQuestion: {question}\\nCorrect Answer: {correct_answer}\\n\\nExplanation:\"\n",
    "#     )\n",
    "#     prompt = template.format(question=question, correct_answer=correct_answer)\n",
    "#     response = llm.complete(prompt)\n",
    "#     return response.text\n",
    "\n",
    "# def parse_questions(response_text, output_parser, n_questions):\n",
    "#     \"\"\"Parse generated questions from response\"\"\"\n",
    "#     questions_data = []\n",
    "#     question_texts = response_text.split(\"\\n\\n\")\n",
    "    \n",
    "#     for q_text in question_texts:\n",
    "#         if not q_text.strip():\n",
    "#             continue\n",
    "            \n",
    "#         q_text = q_text.strip()\n",
    "#         if q_text.startswith(\"```json\"):\n",
    "#             q_text = q_text[7:]\n",
    "#         if q_text.endswith(\"```\"):\n",
    "#             q_text = q_text[:-3]\n",
    "            \n",
    "#         try:\n",
    "#             result = output_parser.parse(q_text)\n",
    "#             questions_data.append(format_question(result))\n",
    "            \n",
    "#             if len(questions_data) >= n_questions:\n",
    "#                 break\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error parsing question: {e}\")\n",
    "#             continue\n",
    "            \n",
    "#     return questions_data[:n_questions]\n",
    "\n",
    "# def format_question(result):\n",
    "#     \"\"\"Format parsed question data\"\"\"\n",
    "#     question = result.question\n",
    "#     correct_answer = result.correct_answer\n",
    "#     options = [\"Option A\", \"Option B\", \"Option C\", \"Option D\"]\n",
    "#     answers = [result.option_a, result.option_b, result.option_c, result.option_d]\n",
    "    \n",
    "#     pre_answer = ['A) ', 'B) ', 'C) ', 'D) ']\n",
    "#     formatted_question = question + '\\n' + \" \".join([pre + \" \" + answer for pre, answer in zip(pre_answer, answers)])\n",
    "    \n",
    "#     correct_option = options[answers.index(correct_answer)]\n",
    "#     explanation = generate_explanation(question, correct_answer)\n",
    "    \n",
    "#     return (formatted_question, options, correct_option, explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from llama_index.core.schema import Node, NodeWithScore, QueryBundle, MediaResource\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from elasticsearch import Elasticsearch\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from llama_index.core.vector_stores.types import VectorStoreQuery\n",
    "\n",
    "class SemanticSearchRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever specialized for semantic search using embeddings (vector search)\"\"\"\n",
    "    def __init__(self, vector_index, embed_model, top_k=4):\n",
    "        self.vector_index = vector_index\n",
    "        self.embed_model = embed_model\n",
    "        self.top_k = top_k\n",
    "        \n",
    "    def _retrieve(self, query_str: str) -> List[NodeWithScore]:\n",
    "        # Add query prefix for better embedding\n",
    "        modified_query = f\"query: {query_str}\"\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embed_model.get_text_embedding(modified_query)\n",
    "        \n",
    "        # Perform vector search\n",
    "        vector_results = self.vector_index.vector_store.query(\n",
    "            query=VectorStoreQuery(\n",
    "                query_embedding=query_embedding,\n",
    "                similarity_top_k=self.top_k\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return [\n",
    "            NodeWithScore(node=node, score=score)\n",
    "            for node, score in zip(vector_results.nodes, vector_results.similarities or [])\n",
    "        ]\n",
    "    \n",
    "class SemanticReranker:\n",
    "    \"\"\"Reranker using semantic similarity\"\"\"\n",
    "    def __init__(self, embed_model):\n",
    "        self.embed_model = embed_model\n",
    "        \n",
    "    def compute_similarity(self, query_embedding: List[float], doc_embedding: List[float]) -> float:\n",
    "        try:\n",
    "            # Convert to torch tensors\n",
    "            query_emb = torch.tensor(query_embedding).reshape(1, -1)\n",
    "            doc_emb = torch.tensor(doc_embedding).reshape(1, -1)\n",
    "            \n",
    "            # Calculate similarity\n",
    "            similarity = torch.nn.functional.cosine_similarity(query_emb, doc_emb, dim=1)\n",
    "\n",
    "            # Convert to float\n",
    "            return float(similarity.item())\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing similarity: {str(e)}\")\n",
    "            return 0.0\n",
    "\n",
    "class KeywordRetriever:\n",
    "    \"\"\"Retriever specialized for keyword-based search using Elasticsearch\"\"\"\n",
    "    \n",
    "    def __init__(self, es_host: str, index_name: str, top_k: int = 4):\n",
    "        try:\n",
    "            self.es_client = Elasticsearch(es_host)\n",
    "            if not self.es_client.ping():\n",
    "                raise ConnectionError(\"Could not connect to Elasticsearch\")\n",
    "        except Exception as e:\n",
    "            raise ConnectionError(f\"Error connecting to Elasticsearch: {str(e)}\")\n",
    "            \n",
    "        self.index_name = index_name\n",
    "        self.top_k = top_k\n",
    "            \n",
    "    def retrieve(self, query_str: str) -> List[Dict]:\n",
    "        \"\"\"Tìm kiếm trên Elasticsearch với full-text search.\"\"\"\n",
    "        try:\n",
    "            search_query = {\n",
    "                \"size\": self.top_k,\n",
    "                \"query\": {\n",
    "                    \"match\": {\n",
    "                        \"content\": {\n",
    "                            \"query\": query_str,\n",
    "                            \"boost\": 1.0\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            results = self.es_client.search(\n",
    "                index=self.index_name,\n",
    "                body=search_query,\n",
    "            )\n",
    "            \n",
    "            return results[\"hits\"][\"hits\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Elasticsearch search: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def retrieve_by_id(self, chunk_ids: List[str]) -> List[Dict]:\n",
    "        \"\"\"Truy xuất các chunks liên quan bằng ID.\"\"\"\n",
    "        if not chunk_ids:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            search_query = {\n",
    "                \"size\": len(chunk_ids),\n",
    "                \"query\": {\n",
    "                    \"terms\": {\n",
    "                        \"id\": chunk_ids\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            results = self.es_client.search(\n",
    "                index=self.index_name,\n",
    "                body=search_query,\n",
    "            )\n",
    "            \n",
    "            return results[\"hits\"][\"hits\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving related chunks: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "class HybridSearchRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever kết hợp Elasticsearch (BM25) và Semantic Search (vector search).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        es_host: str,\n",
    "        index_name: str,\n",
    "        embed_model,\n",
    "        initial_top_k: int = 20,\n",
    "        final_top_k: int = 4\n",
    "    ):\n",
    "        self.keyword_retriever = KeywordRetriever(\n",
    "            es_host=es_host,\n",
    "            index_name=index_name,\n",
    "            top_k=initial_top_k\n",
    "        )\n",
    "        self.reranker = SemanticReranker(embed_model)\n",
    "        self.embed_model = embed_model\n",
    "        self.final_top_k = final_top_k\n",
    "        \n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        # Step 1: Initial retrieval with keywords\n",
    "        bm25_results = self.keyword_retriever.retrieve(query_bundle.query_str)\n",
    "        \n",
    "        # Step 2: Prepare for reranking\n",
    "        modified_query = f\"query: {query_bundle.query_str}\"\n",
    "        query_embedding = self.embed_model.get_text_embedding(modified_query)\n",
    "        \n",
    "        reranked_results = []\n",
    "        \n",
    "        # Step 3: Rerank using semantic similarity\n",
    "        for hit in bm25_results:\n",
    "            content = hit['_source']['content']\n",
    "            \n",
    "            # Get document embedding\n",
    "            doc_embedding = self.embed_model.get_text_embedding(content)\n",
    "            \n",
    "            # Calculate semantic similarity\n",
    "            semantic_score = self.reranker.compute_similarity(\n",
    "                query_embedding,\n",
    "                doc_embedding\n",
    "            )\n",
    "            \n",
    "            # Combine BM25 and semantic scores\n",
    "            es_score = hit['_score']\n",
    "            final_score = (semantic_score + es_score) / 2\n",
    "            \n",
    "            node = Node(\n",
    "                text_resource=MediaResource(text=content),\n",
    "                metadata={\n",
    "                    \"es_score\": es_score,\n",
    "                    \"semantic_score\": semantic_score\n",
    "                }\n",
    "            )\n",
    "            reranked_results.append({\n",
    "                'node': node,\n",
    "                'score': final_score\n",
    "            })\n",
    "\n",
    "        # Step 4: Sort by final score\n",
    "        sorted_results = sorted(\n",
    "            reranked_results,\n",
    "            key=lambda x: x['score'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Get initial top_k results\n",
    "        top_k_results = sorted_results[:self.final_top_k]\n",
    "        \n",
    "        # # Step 1.5: Lấy danh sách related_chunks từ metadata\n",
    "        # related_chunk_ids = set()\n",
    "        # for hit in bm25_results:\n",
    "        #     node_content = json.loads(hit['_source']['metadata']['_node_content'])\n",
    "        #     metadata = node_content.get(\"metadata\", {})\n",
    "        #     related_chunks = metadata.get('related_chunks', \"\")\n",
    "        #     if related_chunks:\n",
    "        #         related_chunk_ids.update(related_chunks.split(\", \"))\n",
    "\n",
    "        # # Truy vấn thêm related_chunks nếu có\n",
    "        # related_chunks = self.keyword_retriever.retrieve_by_id(list(related_chunk_ids))\n",
    "        \n",
    "        # # Gộp kết quả\n",
    "        # all_results = bm25_results + related_chunks\n",
    "        \n",
    "        # Step 5: Get related chunks from top_k results\n",
    "        existing_chunk_ids = set()\n",
    "        related_chunk_ids = set()\n",
    "        for result in top_k_results:\n",
    "            hit = result['node'].metadata.get('es_hit', {})\n",
    "            try:\n",
    "                chunk_id = hit['_source']['id']\n",
    "                existing_chunk_ids.add(chunk_id)\n",
    "                \n",
    "                # Get related chunks\n",
    "                node_content = json.loads(hit['_source']['metadata']['_node_content'])\n",
    "                metadata = node_content.get(\"metadata\", {})\n",
    "                related_chunks = metadata.get('related_chunks', \"\")\n",
    "                if related_chunks:\n",
    "                    related_chunk_ids.update(related_chunks.split(\", \"))\n",
    "            except (json.JSONDecodeError, KeyError, TypeError):\n",
    "                continue\n",
    "            \n",
    "        # Remove any related_chunk_ids that are already in top_k_results\n",
    "        related_chunk_ids = related_chunk_ids - existing_chunk_ids\n",
    "\n",
    "        # Retrieve and process related chunks\n",
    "        final_results = top_k_results\n",
    "        if related_chunk_ids:\n",
    "            related_chunks = self.keyword_retriever.retrieve_by_id(list(related_chunk_ids))\n",
    "            \n",
    "            # Convert related chunks to NodeWithScore format\n",
    "            for hit in related_chunks:\n",
    "                content = hit['_source']['content']\n",
    "                node = Node(\n",
    "                    text_resource=MediaResource(text=content),\n",
    "                    metadata={\n",
    "                        \"es_score\": hit['_score'],\n",
    "                        \"is_related\": True\n",
    "                    }\n",
    "                )\n",
    "                final_results.append({\n",
    "                    'node': node,\n",
    "                    'score': hit['_score']  # Use ES score for related chunks\n",
    "                })\n",
    "\n",
    "        # Return all results without limiting\n",
    "        return [\n",
    "            NodeWithScore(node=item['node'], score=item['score'])\n",
    "            for item in final_results\n",
    "        ]\n",
    "\n",
    "def get_relevant_chunk(\n",
    "    vector_index,\n",
    "    embed_model,\n",
    "    es_host: str,\n",
    "    es_index_name: str,\n",
    "    question_context: str,\n",
    "    top_k: int = 3\n",
    ") -> List[str]:\n",
    "    \"\"\"Truy xuất các đoạn văn bản liên quan bằng hybrid search (BM25 + Semantic Search).\"\"\"\n",
    "    \n",
    "    # Tạo hybrid retriever\n",
    "    hybrid_retriever = HybridSearchRetriever(\n",
    "        es_host=es_host,\n",
    "        index_name=es_index_name,\n",
    "        # vector_index=vector_index,\n",
    "        embed_model=embed_model,\n",
    "        initial_top_k=10,\n",
    "        final_top_k=top_k\n",
    "    )\n",
    "    \n",
    "    # Truy vấn kết quả\n",
    "    results = hybrid_retriever._retrieve(QueryBundle(question_context))\n",
    "\n",
    "    # Trả về các đoạn văn bản phù hợp\n",
    "    return [node.node.text_resource.text for node in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Unit 1\"\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from googletrans import Translator\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def translate_to_english(text):\n",
    "    translator = Translator()\n",
    "    translation = await translator.translate(text, src=\"vi\", dest=\"en\")\n",
    "    return translation.text\n",
    "\n",
    "# Chạy mà không gặp lỗi event loop\n",
    "prompt = asyncio.run(translate_to_english(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_222780/671901355.py:63: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  if not self.es_client.ping():\n",
      "/tmp/ipykernel_222780/671901355.py:86: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  results = self.es_client.search(\n"
     ]
    }
   ],
   "source": [
    "index = load_database()\n",
    "text_chunks = get_relevant_chunk(vector_index=index, \n",
    "                                 embed_model=embed_model,\n",
    "                                 es_host=\"http://localhost:9200\",\n",
    "                                 es_index_name=\"vocabulary\",\n",
    "                                 question_context=prompt, \n",
    "                                 top_k=4\n",
    "                                 )\n",
    "# questions_data = generate_questions_batch(text_chunks, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['passage: Unit: 1. Life stories we admire\\nSection: Getting Started\\nType: list\\n3 Find words and a phrase in 1 with the following meanings.\\n1 a _________ descriptions of things that have happened\\n2 d_________ the end of somebody’s life\\n3 d_________ to giving time, attention, etc. to something\\n4 y_________ the period of time when a person is young',\n",
       " 'passage: Unit: 1. Life stories we admire\\nSection: Getting Started\\nType: text\\nACTIVITY 4\\nAim: To help Ss identify the past simple and the past continuous.\\n•\\tTell Ss to read the summary and check understanding.\\n•\\tAsk Ss to complete the sentences, using words and phrases from the conversation in 1.\\n•\\tCheck answers as a class.\\n•\\tElicit the verb tenses, i.e. past simple and past continuous.\\nKey: 1. wrote\\t2. was working\\t3. was killed\\t4. was doing her duty\\t5. kept',\n",
       " 'passage: Unit: 1. Life stories we admire\\nSection: Getting Started\\nType: text\\nThis unit includes:\\nLANGUAGE\\nPronunciation\\nDiphthongs /eɪ/ and /aʊ/\\nVocabulary\\nPhrases related to life stories\\nGrammar\\nPast simple vs. Past continuous\\nSKILLS\\nReading: Reading for main ideas and specific information in an article about Steve Jobs’ life and achievements\\nSpeaking: Talking about the lives of two national heroes of Viet Nam\\nListening: Listening for main ideas and specific information in a talk about the life of Walt Disney\\nWriting: Synthesising and summarising information to write a biography of Walt Disney\\nCOMMUNICATION AND CULTURE / CLIL\\nEveryday English\\nExpressing pleasure and responding to it\\nCulture/CLIL\\nFamous queens in world history\\nPROJECT\\nDesigning a visual story of a person’s life',\n",
       " 'passage: Unit: 1. Life stories we admire\\nSection: Getting Started\\nType: text\\n4 Complete the sentences based on the conversation.\\nDang Thuy Tram was a young surgeon. She (1) _________ her diary while she (2) _________ in a field hospital during the war. One day, she (3) _________ while she (4) _________ in the jungle. She was only 27 then. An American soldier (5) _________ her diary for many years before returning a copy to her family.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_173960/3379627035.py:20: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.index(index=\"vocabulary\", id=doc_id, body={\"content\": content, \"metadata\": metadata})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu đã đồng bộ vào Elasticsearch!\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Kết nối Elasticsearch\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "# Kết nối ChromaDB\n",
    "collection = chroma_client.get_collection(name=\"unit1_db\")\n",
    "\n",
    "# Lấy tất cả dữ liệu từ ChromaDB\n",
    "data = collection.get(include=[\"documents\", \"metadatas\"]) \n",
    "\n",
    "# Trích xuất ids và documents\n",
    "doc_ids = data[\"ids\"]\n",
    "documents = data[\"documents\"]\n",
    "metadatas = data.get(\"metadatas\", [{}] * len(doc_ids))  # Nếu không có metadata, dùng dict rỗng\n",
    "\n",
    "# Đưa vào Elasticsearch\n",
    "for doc_id, content, metadata in zip(doc_ids, documents, metadatas):\n",
    "    es.index(index=\"vocabulary\", id=doc_id, body={\"content\": content, \"metadata\": metadata})\n",
    "\n",
    "print(\"Dữ liệu đã đồng bộ vào Elasticsearch!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_chromadb_to_es():\n",
    "    \"\"\"Đồng bộ dữ liệu từ ChromaDB sang Elasticsearch, hỗ trợ cập nhật nếu đã tồn tại.\"\"\"\n",
    "    \n",
    "    # Lấy dữ liệu từ ChromaDB (lưu ý không cần 'ids' trong include)\n",
    "    data = collection.get(include=[\"documents\", \"metadatas\"])\n",
    "    \n",
    "    # Trích xuất thông tin\n",
    "    doc_ids = data[\"ids\"]   # IDs luôn có sẵn\n",
    "    documents = data[\"documents\"]\n",
    "    metadatas = data.get(\"metadatas\", [{}] * len(doc_ids))  # Metadata nếu có\n",
    "    \n",
    "    # Danh sách dữ liệu để đẩy vào Elasticsearch\n",
    "    actions = []\n",
    "    \n",
    "    for doc_id, text, metadata in zip(doc_ids, documents, metadatas):\n",
    "        # Kiểm tra nếu tài liệu đã tồn tại\n",
    "        if es.exists(index=\"vocabulary\", id=doc_id):\n",
    "            action = {\n",
    "                \"_op_type\": \"update\",\n",
    "                \"_index\": \"vocabulary\",\n",
    "                \"_id\": doc_id,\n",
    "                \"doc\": {\"content\": text, \"metadata\": metadata}\n",
    "            }\n",
    "        else:\n",
    "            action = {\n",
    "                \"_op_type\": \"index\",\n",
    "                \"_index\": \"vocabulary\",\n",
    "                \"_id\": doc_id,\n",
    "                \"content\": text,\n",
    "                \"metadata\": metadata\n",
    "            }\n",
    "        \n",
    "        actions.append(action)\n",
    "    \n",
    "    # Thực hiện cập nhật hàng loạt với Bulk API để tối ưu hiệu suất\n",
    "    if actions:\n",
    "        bulk(es, actions)\n",
    "        print(f\"Đã đồng bộ {len(actions)} tài liệu vào Elasticsearch.\")\n",
    "    else:\n",
    "        print(\"Không có tài liệu mới để đồng bộ.\")\n",
    "\n",
    "# Chạy đồng bộ\n",
    "sync_chromadb_to_es()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
