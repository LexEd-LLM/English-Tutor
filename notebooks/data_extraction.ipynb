{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cấu hình đường dẫn\n",
    "PDF_DIR = \"/home/buma04/ai-questgen/data/pdf_files/sgk/\"  # thư mục chứa file PDF\n",
    "OUTPUT_IMG_DIR = \"/home/buma04/ai-questgen/data/images_files/sgk/\"  # thư mục lưu ảnh output\n",
    "RESULT_DIR = \"/home/buma04/ai-questgen/data/json_format\"  # thư mục lưu kết quả JSON\n",
    "\n",
    "# Tạo các thư mục nếu chưa tồn tại\n",
    "for dir_path in [PDF_DIR, OUTPUT_IMG_DIR, RESULT_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Khởi tạo Gemini\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "model = genai.GenerativeModel('gemini-2.0-flash-thinking-exp-01-21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction_prompt =\"\"\"\n",
    "You are an expert in extracting structured data from book page images and converting it into a machine-readable JSON format optimized for retrieval-augmented generation (RAG).  \n",
    "\n",
    "# **Task Description**  \n",
    "Extract structured text from a scanned book page while maintaining logical organization, readability, and chunking optimized for vector search. Ensure each chunk contains a complete and meaningful unit of information.  \n",
    "\n",
    "# **Extraction Guidelines**\n",
    "  \n",
    "## 1 **Preserve Text Structure & Chunking**  \n",
    "- Extract all visible text while maintaining **headings, subheadings, paragraphs, and dialogues**.  \n",
    "- **Chunk text logically**:  \n",
    "  - Each chunk must be a self-contained unit (e.g., a paragraph, a list, or a dialogue turn).  \n",
    "  - Avoid overly large chunks that contain multiple ideas.  \n",
    "  - Preserve the **natural reading flow**.  \n",
    "  - Group all contents in structured elements (tables, side notes, boxes, examples, etc.) into a single chunk, preserve the structure as text while maintaining column and row alignment. The text should reflect the table's structure, with rows separated by newline (`\\n`) and columns separated by a delimiter (e.g., tab `\\t` or pipe `|`).\n",
    "  - If a task instruction (exercise prompt) directly refers to a text, keep both in the same chunk.\n",
    "\n",
    "## 2 **Identify & Classify Page Type**  \n",
    "- `cover_page`: Only contains the book title.  \n",
    "- `table_of_contents`: Contains unit titles and corresponding page numbers.  \n",
    "- `unit_start`: Introduces a new unit (extract `unit_title`).  \n",
    "- `unit_content`: Belongs to a unit but does not introduce it (exclude `unit_title`). \n",
    "\n",
    "## 3 **Section Title Assignment**  \n",
    "- Each unit follows a fixed structure:  \n",
    "  `[\"I. GETTING STARTED\", \"II. LANGUAGE\", \"III. READING\", \"IV. SPEAKING\", \"V. LISTENING\", \"VI. WRITING\", \"VII. COMMUNICATION AND CULTURE/CLIL\", \"VIII. LOOKING BACK\", \"PROJECT\"]`.  \n",
    "- If a page does not explicitly contain a section title:  \n",
    "  - **Inherit** `previous_section_title` as `section_title`.  \n",
    "  - **Infer** the most relevant title based on context.  \n",
    "\n",
    "## 4 **Chunking Rules & Metadata**  \n",
    "Each extracted unit of text (chunk) must be stored as:  \n",
    "```json\n",
    "{\n",
    "    \"id\": \"Unique identifier for each chunk\",\n",
    "    \"unit\": \"Unit title\",\n",
    "    \"section\": \"Section title\",\n",
    "    \"type\": \"text | dialogue | image | list\",\n",
    "    \"content\": \"Extracted text (escaped properly for JSON)\",\n",
    "    \"metadata\": {\n",
    "        \"page\": \"Page number\",\n",
    "        \"chunk_type\": \"overview | paragraph | dialogue | list\",\n",
    "        \"speaker\": \"Speaker name (if applicable)\",\n",
    "        \"related_chunks\": \"Comma-seperated string of related chunk IDs\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## 5 **Dialogue Formatting**  \n",
    "- Extract full dialogues as a single chunk instead of splitting into individual turns.\n",
    "- Include all speaker names and their lines in the same chunk, formatted clearly for readability.\n",
    "```json\n",
    "{\n",
    "    \"type\": \"dialogue\",\n",
    "    \"speaker\": \"Mark\",\n",
    "    \"content\": \"Mark: Hi, Nam. Your book must be very interesting. What are you reading?\\nNam: I’m reading a book called *The Diary of Dang Thuy Tram*.\\nMark: Dang Thuy Tram? Who is she?\\nNam: She was born in Hue in 1942.\",\n",
    "    \"metadata\": {\n",
    "        \"page\": 8,\n",
    "        \"chunk_type\": \"dialogue\",\n",
    "        \"related_chunks\": \"unit_1_page_8_3\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "## 6 **Handling Related Chunks**  \n",
    "Each chunk should include \"related_chunks\" in metadata to ensure proper retrieval in multi-turn queries:\n",
    "\n",
    "- For dialogues, store the entire conversation as a single chunk.\n",
    "- Only link related non-dialogue chunks that provide background information.\n",
    "- For paragraphs, related chunks should:\n",
    "  - Reference other chunks discussing the same topic or entity.\n",
    "  - Link supporting details to summary sentences.\n",
    "\n",
    "## 7 **Handling Special Text Features**  \n",
    "- Lists, bullet points, and bold text must be retained.\n",
    "- Newline (\\n) and tab (\\t) characters must be escaped (\\\\n, \\\\t).\n",
    "- Images or captions should be extracted with a description.\n",
    "\n",
    "## 8 **Language & Multilingual Support**  \n",
    "Maintain the original English and Vietnamese text without translation.\n",
    "\n",
    "# **JSON Output Structure**\n",
    "[\n",
    "  {\n",
    "    \"id\": \"unit_1_page_8_1\",\n",
    "    \"unit\": \"Life stories we admire\",\n",
    "    \"section\": \"Getting Started\",\n",
    "    \"type\": \"text\",\n",
    "    \"content\": \"This unit includes: LANGUAGE - Pronunciation: Diphthongs /eɪ/ and /aʊ/, Vocabulary: Phrases related to life stories...\",\n",
    "    \"metadata\": {\n",
    "      \"page\": 8,\n",
    "      \"chunk_type\": \"overview\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"id\": \"unit_1_page_8_5\",\n",
    "    \"unit\": \"Life stories we admire\",\n",
    "    \"section\": \"Getting Started\",\n",
    "    \"type\": \"dialogue\",\n",
    "    \"content\": \"Mark: Hi, Nam. Your book must be very interesting. What are you reading?\\nNam: I’m reading a book called *The Diary of Dang Thuy Tram*.\\nMark: Dang Thuy Tram? Who is she?\\nNam: She was born in Hue in 1942.\",\n",
    "    \"metadata\": {\n",
    "      \"page\": \"8\",\n",
    "      \"chunk_type\": \"dialogue\"\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "Additional Requirements\n",
    "- Accurate text extraction (English & Vietnamese).\n",
    "- Chunking follows a logical, retrievable structure.\n",
    "- No summarization or paraphrasing—text must be extracted exactly as shown.\n",
    "- Consistent metadata tagging to improve searchability.\n",
    "- Maintain reading order and logical flow.\n",
    "\n",
    "Section from Previous Page:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm chuyển PDF sang ảnh\n",
    "def convert_pdf_to_images(pdf_directory, output_dir):\n",
    "    pages_png = []\n",
    "    \n",
    "    for pdf_file in os.listdir(pdf_directory):\n",
    "        if pdf_file.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_directory, pdf_file)\n",
    "            # Chuyển PDF thành ảnh\n",
    "            images = convert_from_path(pdf_path, use_pdftocairo=False, thread_count=15)\n",
    "            # Tạo thư mục output cho từng PDF\n",
    "            pdf_output_dir = os.path.join(output_dir, os.path.splitext(pdf_file)[0])\n",
    "            os.makedirs(pdf_output_dir, exist_ok=True)\n",
    "            # Lưu từng trang thành file ảnh\n",
    "            for page_num, image in enumerate(images):\n",
    "                page_filename = f\"page-{str(page_num + 1).zfill(3)}.png\"\n",
    "                full_path = os.path.join(pdf_output_dir, page_filename)\n",
    "                image.thumbnail((768, 768), Image.Resampling.LANCZOS)\n",
    "                image.save(full_path)\n",
    "                pages_png.append(full_path)\n",
    "                \n",
    "    return pages_png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm trích xuất thông tin từ ảnh using Gemini\n",
    "previous_section_title = None  # Nội dung của trang trước đó\n",
    "\n",
    "def extract_page_content(image_path, model, system_prompt):\n",
    "    global previous_section_title  # Giữ nội dung của trang trước đó giữa các lần gọi hàm\n",
    "    # Đọc ảnh\n",
    "    image = Image.open(image_path)\n",
    "    image.thumbnail((768, 768), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Tạo prompt với context từ trang trước\n",
    "    previous_section_title = previous_section_title if previous_section_title else \"No previous page available\"\n",
    "    current_prompt = system_prompt + previous_section_title\n",
    "    \n",
    "    # Gọi Gemini API\n",
    "    response = model.generate_content([\n",
    "        current_prompt,\n",
    "        image\n",
    "    ])\n",
    "\n",
    "    clean_output = response.text.strip(\"```json\").strip(\"```\").strip()\n",
    "    \n",
    "    # Parse JSON response\n",
    "    result = json.loads(clean_output)\n",
    "    previous_section_title = result[-1]['section']  # Lưu section của trang này để sử dụng cho trang tiếp theo\n",
    "\n",
    "    # Lưu kết quả\n",
    "    output_file = os.path.join(\n",
    "        RESULT_DIR, \n",
    "        f\"{os.path.basename(image_path).replace('.png', '.json')}\"\n",
    "    )\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing images found. Converting PDFs to images...\n",
      "Generated 33 images\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Chạy toàn bộ quy trình\n",
    "# 1. Chuyển PDF sang ảnh\n",
    "# print(\"Converting PDFs to images...\")\n",
    "# image_paths = convert_pdf_to_images(PDF_DIR, OUTPUT_IMG_DIR)\n",
    "# print(f\"Generated {len(image_paths)} images\")\n",
    "def get_existing_images(output_dir):\n",
    "    \"\"\"Lấy danh sách các file ảnh đã tồn tại\"\"\"\n",
    "    image_paths = []\n",
    "    for root, _, files in os.walk(output_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.png'):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    return sorted(image_paths)\n",
    "\n",
    "# Kiểm tra xem đã có ảnh trong thư mục output chưa\n",
    "image_paths = get_existing_images(OUTPUT_IMG_DIR)\n",
    "\n",
    "if not image_paths:\n",
    "    # Nếu chưa có ảnh, thực hiện chuyển đổi PDF\n",
    "    print(\"No existing images found. Converting PDFs to images...\")\n",
    "    image_paths = convert_pdf_to_images(PDF_DIR, OUTPUT_IMG_DIR)\n",
    "    print(f\"Generated {len(image_paths)} images\")\n",
    "else:\n",
    "    print(f\"Found {len(image_paths)} existing images\")\n",
    "\n",
    "\n",
    "# # 2. Trích xuất thông tin từ từng ảnh\n",
    "# print(\"\\nExtracting content from images...\")\n",
    "# results = []\n",
    "# import time\n",
    "\n",
    "# for image_path in image_paths:\n",
    "#     while True:\n",
    "#         print(f\"Processing {image_path}...\")\n",
    "#         result = extract_page_content(image_path, model, system_instruction_prompt)\n",
    "#         if result:\n",
    "#             results.append(result)\n",
    "#         break  # Break the while loop if successful\n",
    "\n",
    "\n",
    "# print(f\"\\nProcessed {len(results)} pages successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
