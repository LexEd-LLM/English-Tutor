{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    Settings,\n",
    "    Document,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "from llama_index.core.chat_engine.types import ChatMode\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "\n",
    "from prompts import SYSTEM_PROMPT, USER_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM + Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(\n",
    "    model='llama3.1:8b',\n",
    "    )\n",
    "\n",
    "lc_embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/multilingual-e5-small\"\n",
    ")\n",
    "embed_model = LangchainEmbedding(lc_embed_model)\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "# Set the following API Keys in the Python environment. Will be used later.\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "Settings.llm = OpenAI(temperature=0, model=\"gpt-4o\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"../chromadb\")\n",
    "chroma_collection = chroma_client.get_collection(\"unit1_db\")\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    embed_model=embed_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tạo QA Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_text_qa_msgs = [\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"user\", USER_PROMPT),\n",
    "]\n",
    "text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Khởi tạo Chat Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle\n",
    "from llama_index.core.vector_stores.types import VectorStoreQuery\n",
    "from typing import List\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    def __init__(self, vector_index, embed_model, top_k=4):\n",
    "        self.vector_index = vector_index\n",
    "        self.embed_model = embed_model\n",
    "        self.top_k = top_k\n",
    "        \n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        # Tùy chỉnh query text trước khi embedding\n",
    "        modified_query = f\"query: {query_bundle.query_str}\"\n",
    "        \n",
    "        # Tạo embedding cho query\n",
    "        query_embedding = self.embed_model.get_text_embedding(modified_query)\n",
    "        \n",
    "        # Tạo vector store query\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self.top_k\n",
    "        )\n",
    "        \n",
    "        # Thực hiện tìm kiếm vector\n",
    "        query_result = self.vector_index.vector_store.query(\n",
    "            query=vector_store_query\n",
    "        )\n",
    "        \n",
    "        # Chuyển đổi kết quả thành List[NodeWithScore]\n",
    "        nodes_with_scores = []\n",
    "        for node, score in zip(query_result.nodes, query_result.similarities or []):\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "            \n",
    "        return nodes_with_scores\n",
    "    \n",
    "from llama_index.core.chat_engine import ContextChatEngine\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core.chat_engine.types import AgentChatResponse\n",
    "from llama_index.core.tools import ToolOutput\n",
    "\n",
    "class CustomChatEngine(ContextChatEngine):\n",
    "    \"\"\"Custom chat engine that extends ContextChatEngine\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        retriever,\n",
    "        llm,\n",
    "        memory,\n",
    "        text_qa_template,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # Tạo prefix_messages từ system prompt trong template\n",
    "        system_prompt = text_qa_template.message_templates[0].blocks[0].text\n",
    "        prefix_messages = [\n",
    "            ChatMessage(content=system_prompt, role=llm.metadata.system_role)\n",
    "        ]\n",
    "        \n",
    "        super().__init__(\n",
    "            retriever=retriever,\n",
    "            llm=llm,\n",
    "            memory=memory,\n",
    "            prefix_messages=prefix_messages,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.text_qa_template = text_qa_template\n",
    "\n",
    "    def _is_answer(self, message: str) -> bool:\n",
    "        \"\"\"Kiểm tra xem message có phải là câu trả lời không\"\"\"\n",
    "        # Lấy lịch sử chat\n",
    "        chat_history = self._memory.get()\n",
    "        if not chat_history:\n",
    "            return False\n",
    "            \n",
    "        # Lấy tin nhắn cuối cùng của AI\n",
    "        last_ai_message = None\n",
    "        for msg in reversed(chat_history):\n",
    "            if msg.role == MessageRole.ASSISTANT:\n",
    "                last_ai_message = msg.content\n",
    "                break\n",
    "                \n",
    "        if not last_ai_message:\n",
    "            return False\n",
    "            \n",
    "        # Tạo prompt để phân tích\n",
    "        analysis_prompt = f\"\"\"Analyze if the following user message is an answer to the previous question/exercise:\n",
    "            Previous AI message: {last_ai_message}\n",
    "            User message: {message}\n",
    "\n",
    "            Return only \"true\" if it's an answer, or \"false\" if it's a new question/request.\n",
    "        \"\"\"\n",
    "\n",
    "        # Phân tích với LLM\n",
    "        response = self._llm.complete(analysis_prompt)\n",
    "        print(response)\n",
    "        return \"false\" not in str(response).strip().lower()\n",
    "\n",
    "    def chat(self, message: str) -> AgentChatResponse:\n",
    "        # Kiểm tra xem message có phải là câu trả lời không\n",
    "        is_answer = self._is_answer(message)\n",
    "        print(is_answer)\n",
    "        \n",
    "        # Chỉ truy xuất nodes nếu không phải là câu trả lời\n",
    "        if not is_answer:\n",
    "            retrieved_nodes = self._get_nodes(message)\n",
    "        else:\n",
    "            retrieved_nodes = []\n",
    "        \n",
    "        # Tạo context từ retrieved documents\n",
    "        context_str = \"\\n\\n\".join([node.text for node in retrieved_nodes])\n",
    "        \n",
    "        # Tạo prompt với template\n",
    "        prompt = self.text_qa_template.format(\n",
    "            context_str=context_str,\n",
    "            chat_history=self._memory.get(),\n",
    "            question=message\n",
    "        )\n",
    "        \n",
    "        # Gọi LLM để generate response\n",
    "        response = self._llm.complete(prompt)\n",
    "        \n",
    "        # Cập nhật memory\n",
    "        user_message = ChatMessage(role=MessageRole.USER, content=message)\n",
    "        ai_message = ChatMessage(role=MessageRole.ASSISTANT, content=str(response))\n",
    "        self._memory.put(user_message)\n",
    "        self._memory.put(ai_message)\n",
    "        \n",
    "        # Trả về AgentChatResponse\n",
    "        return AgentChatResponse(\n",
    "            response=str(response),\n",
    "            sources=[\n",
    "                ToolOutput(\n",
    "                    tool_name=\"retriever\",\n",
    "                    content=str(retrieved_nodes),\n",
    "                    raw_input={\"message\": message},\n",
    "                    raw_output=retrieved_nodes,\n",
    "                )\n",
    "            ],\n",
    "            source_nodes=retrieved_nodes,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMemoryBuffer.from_defaults(token_limit=2048)\n",
    "\n",
    "# Khởi tạo custom retriever\n",
    "custom_retriever = CustomRetriever(\n",
    "    vector_index=index,\n",
    "    embed_model=embed_model,\n",
    "    top_k=8\n",
    ")\n",
    "\n",
    "# Khởi tạo custom chat engine\n",
    "chat_engine = CustomChatEngine(\n",
    "    retriever=custom_retriever,\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    text_qa_template=text_qa_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hỏi đáp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG pipeline\n",
    "def ask_question(question):\n",
    "    response = chat_engine.chat(question)\n",
    "    print(\"Q:\", question)\n",
    "    print(\"\\nA:\", response)\n",
    "    print(f\"\\nTotal Retrieved Nodes: {len(response.source_nodes)}\")\n",
    "    print(\"\\nRetrieved Nodes:\")\n",
    "    \n",
    "    for i, node in enumerate(response.source_nodes, 1):\n",
    "        print(f\"\\nNode #{i}\")\n",
    "        print(f\"Score: {node.score:.4f}\")\n",
    "        print(f\"ID: {node.metadata.get('id')}\")\n",
    "        print(f\"Unit: {node.metadata.get('unit')}\")\n",
    "        print(f\"Section: {node.metadata.get('section')}\")\n",
    "        print(f\"Type: {node.metadata.get('type', 'N/A')}\")\n",
    "        print(f\"Content Preview: {node.text[:200]}...\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions = [\n",
    "#     \"Hãy cho tôi 10 câu hỏi từ nội dung đoạn hội thoại trong phần Getting Started Unit 1 để tôi ôn tập\",\n",
    "# ]\n",
    "\n",
    "# for q in questions:\n",
    "#     ask_question(q)\n",
    "#     print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu chat với AI Assistant (gõ 'quit' hoặc 'exit' để thoát)\n",
      "--------------------------------------------------\n",
      "False\n",
      "\n",
      "The user message is asking for the creation of 10 fill-in-the-blank exercises for vocabulary practice in Unit 1, which is unrelated to the previous multiple-choice question. It does not attempt to answer the original question.\n",
      "False\n",
      "\n",
      "Giáo viên: Xin chào các em! Hôm nay, chúng ta sẽ tập trung vào từ vựng trong phần \"Life stories we admire\". Hãy đọc kỹ và chọn đáp án đúng!\n",
      "\n",
      "**Câu hỏi 1:** A _________ descriptions of things that have happened.\n",
      "\n",
      "A) accounts\n",
      "B) death\n",
      "C) devoting\n",
      "D) youth\n",
      "\n",
      "Hãy chọn đáp án đúng!\n",
      "True\n",
      "True\n",
      "\n",
      "Giáo viên: Tôi xin chào mừng các em học sinh lớp 12! Hôm nay, chúng ta sẽ tập trung vào việc cải thiện kỹ năng từ vựng bằng cách giải quyết các bài tập liên quan.\n",
      "\n",
      "Bài tập đầu tiên:\n",
      "\n",
      "Multiple Choice Question:\n",
      "\n",
      "What is the meaning of the word \"skeptical\" in the following sentence?\n",
      "\n",
      "\"The tourists were skeptical about the local legend.\"\n",
      "\n",
      "A) tin tưởng\n",
      "B) nghi ngờ\n",
      "C) hứng thú\n",
      "D) lo lắng\n",
      "\n",
      "Hãy chọn đáp án phù hợp.\n"
     ]
    }
   ],
   "source": [
    "def interactive_chat():\n",
    "    print(\"Bắt đầu chat với AI Assistant (gõ 'quit' hoặc 'exit' để thoát)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Nhận input từ người dùng\n",
    "    user_input = \"Em muốn cô tạo ra 10 câu điền từ để ôn tập về từ vựng trong Unit 1\"\n",
    "\n",
    "    # Xử lý câu trả lời\n",
    "    response = chat_engine.chat(user_input)\n",
    "    \n",
    "    # In kết quả\n",
    "    print(\"\\nGiáo viên:\", response.response)\n",
    "    \n",
    "    user_input = \"1. A\"\n",
    "    \n",
    "    # Xử lý câu trả lời\n",
    "    response = chat_engine.chat(user_input)\n",
    "    \n",
    "    # In kết quả\n",
    "    print(\"\\nGiáo viên:\", response.response)\n",
    "\n",
    "# Sử dụng hàm\n",
    "interactive_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu chat với AI Assistant (gõ 'quit' hoặc 'exit' để thoát)\n",
      "--------------------------------------------------\n",
      "False\n",
      "\n",
      "Giáo viên: Let's begin our vocabulary exercise based on the passage.\n",
      "\n",
      "**Câu hỏi 1:** Complete the sentence:\n",
      "Dang Thuy Tram wrote about/operated on injured soldiers during the war.\n",
      "\n",
      "A) She gave medical treatment to them.\n",
      "B) She was a doctor who operated on many patients.\n",
      "C) She kept their stories in her diary.\n",
      "D) She helped those soldiers by writing about them.\n",
      "\n",
      "Hãy chọn đáp án đúng!\n",
      "False\n",
      "\n",
      "The user is answering the exercise with option A), but the format and phrasing of their response (\"1. A\") suggests they are unsure about how to submit their answer, rather than providing additional context or asking a new question.\n",
      "False\n",
      "\n",
      "Giáo viên: Được rồi! Let's start with a multiple-choice question.\n",
      "\n",
      "**Question 1:** What does \"accounts\" mean in the context of life stories?\n",
      "\n",
      "A) Mô tả về một người\n",
      "B) Những câu chuyện về cuộc sống\n",
      "C) Những bài viết về các sự kiện quan trọng\n",
      "D) Các chi tiết trong một cuốn sách\n",
      "\n",
      "Hãy chọn đáp án của bạn!\n"
     ]
    }
   ],
   "source": [
    "def interactive_chat():\n",
    "    print(\"Bắt đầu chat với AI Assistant (gõ 'quit' hoặc 'exit' để thoát)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Nhận input từ người dùng\n",
    "    user_input = \"Em muốn cô tạo ra 10 câu điền từ để ôn tập về từ vựng trong Unit 1\"\n",
    "\n",
    "    # Xử lý câu trả lời\n",
    "    response = chat_engine.chat(user_input)\n",
    "    \n",
    "    # In kết quả\n",
    "    print(\"\\nGiáo viên:\", response.response)\n",
    "    \n",
    "    user_input = \"1. A\"\n",
    "    \n",
    "    # Xử lý câu trả lời\n",
    "    response = chat_engine.chat(user_input)\n",
    "    \n",
    "    # In kết quả\n",
    "    print(\"\\nGiáo viên:\", response.response)\n",
    "\n",
    "# Sử dụng hàm\n",
    "interactive_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
