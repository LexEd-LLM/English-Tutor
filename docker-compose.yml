services:
  backend:
    build:
      context: .
      dockerfile: docker/backend/Dockerfile
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - REDIS_URL=redis-server:6379
    volumes:
      - backend-media:/app/backend/media
    env_file:
      - backend/.env
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health"]
      interval: 5s
      timeout: 2s
      retries: 120
    restart: on-failure
  
  frontend:
    build:
      context: .
      dockerfile: docker/frontend/Dockerfile
      args:
        NEXT_PUBLIC_BACKEND_URL: http://backend:8000
        BACKEND_URL: http://backend:8000
    ports:
      - "11954:3000"
    env_file:
      - frontend/.env
    environment:
      - NEXT_PUBLIC_BACKEND_URL=http://backend:8000
      - BACKEND_URL=http://backend:8000
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - app-network
    restart: on-failure

  # vllm:
  #   build:
  #     context: .
  #     dockerfile: docker/vllm/Dockerfile.vllm-fixed
  #   image: vllm-patched:v0.8.2
  #   container_name: gemma-3-12b-it-int4-awq
  #   runtime: nvidia
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=all
  #     - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
  #   volumes:
  #     - /home/buma04/weights_llm:/models
  #   command: >
  #     --model /models/gemma-3-12b-it-int4-awq
  #     --served_model_name gemma-3-12b
  #     --max-model-len 8096
  #     --tensor-parallel-size 2
  #     --max-num-seqs 20
  #     --gpu-memory-utilization 0.7
  #     --generation-config vllm
  #   ports:
  #     - "8800:8000"
  #   ipc: host
  #   networks:
  #     - app-network
  #   restart: on-failure

  redis-server:
    image: redis:latest
    networks:
      - app-network
    restart: unless-stopped

volumes:
  backend-media:
  postgres-data:

networks:
  app-network:
    driver: bridge